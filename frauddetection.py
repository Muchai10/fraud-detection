# -*- coding: utf-8 -*-
"""frauddetection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aWawYoi9eRkK7aqIYnBDOkUmXYjSAxqN

# MoneySafe AI - Fraud Detection In Mobile Money & Bank Transactions

## 1. Project Setup and Importing Necessary Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# Import essential libraries for general processing
import numpy as np
import pandas as pd

#libraries for data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Import scikit-learn modules for preprocessing, modeling, and evaluation
# libraries for machine learning
# preprocessing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
# modelling
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
#evaluation
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Set visualization style
sns.set_style('whitegrid')
# %matplotlib inline

"""## 2. Data Loading and Initial Inspection.

  First I will load the dataset and perform an initial inspection to understand its structure, data types, and identify any immediate issues like missing values.

  I will print the first five rows to understand the structure.
  Then get a summary of the dataset to identify the data types and check for duplicate data.
"""

# Load the dataset from the provided CSV file
df = pd.read_csv("/content/drive/MyDrive/paysim.csv")

# Display the first 5 rows to get a feel for the data
print("First 5 rows of the dataset:")
display(df.head())

# Get a summary of the dataframe, including data types and non-null counts
print("\nDataset Information:")
df.info()

# Check for duplicate rows
duplicate_rows = df.duplicated().sum()
print(f"\nNumber of duplicate rows: {duplicate_rows}")

if duplicate_rows > 0:
    print("Dropping duplicate rows...")
    df.drop_duplicates(inplace=True)
    print(f"Number of rows after dropping duplicates: {df.shape[0]}")

""" ## Handle missing values

 Here, I identified and addressed the missing values. I chose to handle them through the deletion technique.

"""

# Check for the number of missing values in each column
print("\nMissing Values Count:")
print(df.isnull().sum())

# Drop rows with missing values
df.dropna(inplace=True)

# Verify that missing values have been removed
print("\nMissing values after dropping rows:")
print(df.isnull().sum())

"""## Convert data types
I transformed categorical variables into numerical formats using
label encoding.
"""

# Apply Label Encoding to the 'type' column
label_encoder = LabelEncoder()
df['type'] = label_encoder.fit_transform(df['type'])

# Drop 'nameOrig' and 'nameDest' columns due to high cardinality
df = df.drop(['nameOrig', 'nameDest'], axis=1)

# Display the first few rows of the modified DataFrame
print("DataFrame after encoding 'type' and dropping 'nameOrig' and 'nameDest':")
display(df.head())

# Display the data types to confirm changes
print("\nData types after transformations:")
df.info()

"""## Normalize & Scale Data"""

# Separate features (X) and target (y)
X = df.drop('isFraud', axis=1)
y = df['isFraud']

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit and transform the features
X_scaled = scaler.fit_transform(X)

# Convert the scaled features back to a DataFrame (optional, but good for inspection)
X_scaled = pd.DataFrame(X_scaled, columns=X.columns)

# Display the first few rows of the scaled features
print("Scaled Features:")
display(X_scaled.head())

"""# Exploratory Data Analysis (EDA)
 This includes visualizing data using histograms, scatter plots, and heatmaps, analyzing statistical properties, and identifying anomalies.
"""

# Select numerical columns for histograms
numerical_cols = df.select_dtypes(include=np.number).columns

# Plot histograms for numerical columns
df[numerical_cols].hist(figsize=(15, 10))
plt.tight_layout()
plt.suptitle('Histograms of Numerical Features', y=1.02)
plt.show()

"""Scatter plots to examine the relationships between amount and the balance columns.


"""

# Create scatter plots to examine the relationships between 'amount' and balance columns
balance_cols = ['oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']
plt.figure(figsize=(15, 10))
for i, col in enumerate(balance_cols):
    plt.subplot(2, 2, i + 1)
    sns.scatterplot(x='amount', y=col, data=df)
    plt.title(f'Amount vs {col}')
plt.tight_layout()
plt.suptitle('Scatter Plots of Amount vs Balance Columns', y=1.02)
plt.show()

"""A heatmap of the correlation matrix for all numerical columns in df to visualize the relationships between all pairs of features.


"""

# Generate a heatmap of the correlation matrix
plt.figure(figsize=(10, 8))
correlation_matrix = df[numerical_cols].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Numerical Features')
plt.show()

"""## Analyze statistics

"""

# Calculate and display descriptive statistics for numerical columns
print("Descriptive Statistics for Numerical Columns:")
display(df.describe())

# Calculate and display the value counts for the target variable 'isFraud'
print("\nValue Counts for 'isFraud':")
display(df['isFraud'].value_counts())

# Display the percentage of fraudulent transactions
fraud_percentage = (df['isFraud'].sum() / df.shape[0]) * 100
print(f"\nPercentage of fraudulent transactions: {fraud_percentage:.2f}%")

"""## Identify anomalies

I looked for outliers using visualizations like box plots or by examining extreme values.

"""

# Create box plots for numerical columns to identify outliers
numerical_cols = df.select_dtypes(include=np.number).columns
plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_cols):
    plt.subplot(3, 3, i + 1)
    sns.boxplot(y=df[col])
    plt.title(f'Box plot of {col}')
plt.tight_layout()
plt.suptitle('Box Plots of Numerical Features', y=1.02)
plt.show()

"""# Feature Engineering
I performed feature engineering on the dataset to create new features that might be relevant for fraud detection, handle outliers, and prepare the data for model training.
"""

# Create a new feature called is_zero_balance_after_orig
df['is_zero_balance_after_orig'] = (df['newbalanceOrig'] == 0).astype(int)

# Create a new feature called is_zero_balance_after_dest
df['is_zero_balance_after_dest'] = (df['newbalanceDest'] == 0).astype(int)

# Create a new feature called orig_balance_diff
df['orig_balance_diff'] = df['oldbalanceOrg'] - df['newbalanceOrig']

# Create a new feature called dest_balance_diff
df['dest_balance_diff'] = df['oldbalanceDest'] - df['newbalanceDest']

# Display the first few rows of the DataFrame to show the newly created features
print("DataFrame with new features:")
display(df.head())

# Separate the features (X) and the target variable (y)
X = df.drop('isFraud', axis=1)
y = df['isFraud']

# Initialize a StandardScaler object
scaler = StandardScaler()

# Fit and transform the features (X) using the scaler
X_scaled = scaler.fit_transform(X)

# Convert the scaled features back to a DataFrame for easier inspection
X_scaled = pd.DataFrame(X_scaled, columns=X.columns)

# Display the first few rows of the scaled features DataFrame
print("Scaled Features:")
display(X_scaled.head())

"""## Model Selection, Training & Evaluation

Train and evaluate Logistic Regression and Decision Tree models for fraud detection, considering class imbalance for Decision Tree.

### Split data into training and testing sets.
"""

# Split the scaled data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Print the shapes of the resulting sets to verify the split
print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

# Instantiate a Logistic Regression model
logistic_model = LogisticRegression(random_state=42)

# Instantiate a Decision Tree Classifier model with class weight balancing
decision_tree_model = DecisionTreeClassifier(random_state=42, class_weight='balanced')

# Print the instantiated models to confirm
print("Logistic Regression Model:", logistic_model)
print("Decision Tree Model:", decision_tree_model)

"""### Train model


"""

# Train the Logistic Regression model
print("Training Logistic Regression model...")
logistic_model.fit(X_train, y_train)
print("Logistic Regression model trained.")

# Train the Decision Tree model
print("Training Decision Tree model...")
decision_tree_model.fit(X_train, y_train)
print("Decision Tree model trained.")

"""### Evaluate the performance of the models using appropriate metrics for imbalanced datasets (e.g., precision, recall, F1-score).

"""

# Make predictions on the test set using the trained models
y_pred_logistic = logistic_model.predict(X_test)
y_pred_decision_tree = decision_tree_model.predict(X_test)

# Evaluate the performance of the Logistic Regression model
accuracy_logistic = accuracy_score(y_test, y_pred_logistic)
precision_logistic = precision_score(y_test, y_pred_logistic)
recall_logistic = recall_score(y_test, y_pred_logistic)
f1_logistic = f1_score(y_test, y_pred_logistic)

# Print evaluation metrics for Logistic Regression
print("Logistic Regression Model Evaluation:")
print(f"  Accuracy: {accuracy_logistic:.4f}")
print(f"  Precision: {precision_logistic:.4f}")
print(f"  Recall: {recall_logistic:.4f}")
print(f"  F1-Score: {f1_logistic:.4f}")

# Evaluate the performance of the Decision Tree model
accuracy_decision_tree = accuracy_score(y_test, y_pred_decision_tree)
precision_decision_tree = precision_score(y_test, y_pred_decision_tree)
recall_decision_tree = recall_score(y_test, y_pred_decision_tree)
f1_decision_tree = f1_score(y_test, y_pred_decision_tree)

# Print evaluation metrics for Decision Tree
print("\nDecision Tree Model Evaluation:")
print(f"  Accuracy: {accuracy_decision_tree:.4f}")
print(f"  Precision: {precision_decision_tree:.4f}")
print(f"  Recall: {recall_decision_tree:.4f}")
print(f"  F1-Score: {f1_decision_tree:.4f}")

# Calculate and print confusion matrices
print("\nConfusion Matrix for Logistic Regression:")
display(confusion_matrix(y_test, y_pred_logistic))

print("\nConfusion Matrix for Decision Tree:")
display(confusion_matrix(y_test, y_pred_decision_tree))

"""## Model optimization and reporting.
I tuned the hyperparameters of the models to improve performance.

"""

from sklearn.model_selection import RandomizedSearchCV

# Define a parameter grid for the Decision Tree model
param_dist = {
    'max_depth': [10, 20, 30, 40, 50, None],
    'min_samples_split': [2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 4, 8],
    'criterion': ['gini', 'entropy']
}

# Initialize RandomizedSearchCV
# Using 'f1' as the scoring metric is suitable for imbalanced datasets
random_search = RandomizedSearchCV(decision_tree_model, param_distributions=param_dist, n_iter=10, cv=3, scoring='f1', random_state=42, n_jobs=-1)

# Fit RandomizedSearchCV to the training data
print("Starting Randomized Search for Decision Tree tuning...")
random_search.fit(X_train, y_train)
print("Randomized Search finished.")

# Print the best hyperparameters found
print("\nBest hyperparameters found:")
print(random_search.best_params_)

# Train a new Decision Tree model with the best hyperparameters
best_decision_tree_model = random_search.best_estimator_

# Evaluate the tuned Decision Tree model on the test set
y_pred_tuned_decision_tree = best_decision_tree_model.predict(X_test)

accuracy_tuned_decision_tree = accuracy_score(y_test, y_pred_tuned_decision_tree)
precision_tuned_decision_tree = precision_score(y_test, y_pred_tuned_decision_tree)
recall_tuned_decision_tree = recall_score(y_test, y_pred_tuned_decision_tree)
f1_tuned_decision_tree = f1_score(y_test, y_pred_tuned_decision_tree)

# Print evaluation metrics for the tuned Decision Tree
print("\nTuned Decision Tree Model Evaluation:")
print(f"  Accuracy: {accuracy_tuned_decision_tree:.4f}")
print(f"  Precision: {precision_tuned_decision_tree:.4f}")
print(f"  Recall: {recall_tuned_decision_tree:.4f}")
print(f"  F1-Score: {f1_tuned_decision_tree:.4f}")

# Compare performance to the untuned model
print("\nComparison with Untuned Decision Tree Model:")
print(f"  Untuned F1-Score: {f1_decision_tree:.4f}")
print(f"  Tuned F1-Score: {f1_tuned_decision_tree:.4f}")

# Decide whether to replace the untuned model
if f1_tuned_decision_tree > f1_decision_tree:
    print("\nTuned Decision Tree model shows improved F1-score. Replacing untuned model.")
    decision_tree_model = best_decision_tree_model
else:
    print("\nTuned Decision Tree model did not significantly improve F1-score. Keeping untuned model.")

"""## Summary:

### Data Analysis Key Findings

* The dataset was successfully split into training (80%) and testing (20%) sets, resulting in 2,513,214 training samples and 628,304 testing samples, each with 12 features.
* Both Logistic Regression and Decision Tree models were selected and instantiated for fraud detection, with the Decision Tree using `class_weight='balanced'` to address class imbalance.
* Both models were successfully trained on the training data.
* Evaluation metrics for the Logistic Regression model on the test set were: Accuracy: 0.9995, Precision: 0.9482, Recall: 0.4533, F1-Score: 0.6134.
* Evaluation metrics for the untuned Decision Tree model on the test set were: Accuracy: 0.9997, Precision: 0.8242, Recall: 0.7771, F1-Score: 0.8000.
* Hyperparameter tuning for the Decision Tree using `RandomizedSearchCV` resulted in slightly improved performance, with the tuned model achieving an F1-Score of 0.8124 on the test set.
* The tuned Decision Tree model was selected as the preferred model due to its improved F1-score compared to the untuned version.

### Insights or Next Steps

* The Decision Tree model with balanced class weights and tuned hyperparameters demonstrates better performance in identifying fraudulent transactions compared to Logistic Regression, based on the F1-score.
* Further investigation could involve evaluating other models suitable for imbalanced data (e.g., Random Forest, Gradient Boosting) or exploring techniques like oversampling or undersampling to potentially improve performance further.
"""